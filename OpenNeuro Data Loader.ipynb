{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18c1c0e-9741-4206-a782-b5f2114bb937",
   "metadata": {},
   "source": [
    "# OpenNeuro Data Loader\n",
    "A data loader for open neuro MRI datasets https://openneuro.org/\n",
    "\n",
    "Getting usable data from open neuro was more difficult than it should be. I aim to create a 3 part system to expedite this process.\n",
    "\n",
    "The architecture is as follows:\n",
    "1. Given a dataset ID (ds#######) download the dataset to a specified folder and extract it using datalad\n",
    "1. A 'patient' class to hold data relevant to model training as well as data related to the patient\n",
    "1. A dataset class that has various dataset-related methods (preprocessing, train-val-test splits or stratified k-fold cross validation, ect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6701adc0-67ac-4d5f-9235-7b8a8f3600b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Todos\n",
    "1. Using datalad and git, download dataset\n",
    "1. Figure out memory measuring tool\n",
    "1. Load batch of n scans based on available memory\n",
    "1. Create generator of m batches of n scans which load on demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b84ed6-7239-43c8-9bb2-639e6d4d4a09",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95005cd-94a1-4afb-9a31-6ebb1edf4356",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nipy\n",
      "  Downloading nipy-0.6.1-cp39-cp39-win_amd64.whl (3.0 MB)\n",
      "     ---------------------------------------- 3.0/3.0 MB 8.4 MB/s eta 0:00:00\n",
      "Collecting nibabel\n",
      "  Downloading nibabel-5.3.3-py3-none-any.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 69.9 MB/s eta 0:00:00\n",
      "Collecting nilearn\n",
      "  Downloading nilearn-0.12.1-py3-none-any.whl (12.7 MB)\n",
      "     --------------------------------------- 12.7/12.7 MB 50.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.13.1-cp39-cp39-win_amd64.whl (46.2 MB)\n",
      "     --------------------------------------- 46.2/46.2 MB 65.6 MB/s eta 0:00:00\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.9.4-cp39-cp39-win_amd64.whl (7.8 MB)\n",
      "     ---------------------------------------- 7.8/7.8 MB 50.1 MB/s eta 0:00:00\n",
      "Collecting transforms3d\n",
      "  Downloading transforms3d-0.4.2-py3-none-any.whl (1.4 MB)\n",
      "     ---------------------------------------- 1.4/1.4 MB 91.1 MB/s eta 0:00:00\n",
      "Collecting sympy>=1.9\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 6.3/6.3 MB 57.7 MB/s eta 0:00:00\n",
      "Collecting importlib-resources>=5.12\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: packaging>=20 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nibabel->-r requirements.txt (line 2)) (26.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nibabel->-r requirements.txt (line 2)) (4.15.0)\n",
      "Collecting lxml\n",
      "  Downloading lxml-6.0.2-cp39-cp39-win_amd64.whl (4.0 MB)\n",
      "     ---------------------------------------- 4.0/4.0 MB 64.9 MB/s eta 0:00:00\n",
      "Collecting pandas>=2.2.0\n",
      "  Downloading pandas-2.3.3-cp39-cp39-win_amd64.whl (11.4 MB)\n",
      "     --------------------------------------- 11.4/11.4 MB 50.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.25.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nilearn->-r requirements.txt (line 3)) (2.32.5)\n",
      "Collecting scikit-learn>=1.4.0\n",
      "  Downloading scikit_learn-1.6.1-cp39-cp39-win_amd64.whl (11.2 MB)\n",
      "     --------------------------------------- 11.2/11.2 MB 59.5 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "     ------------------------------------- 309.1/309.1 KB 18.7 MB/s eta 0:00:00\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.60.2-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 96.1 MB/s eta 0:00:00\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.7-cp39-cp39-win_amd64.whl (55 kB)\n",
      "     ---------------------------------------- 55.8/55.8 KB ? eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.0-cp39-cp39-win_amd64.whl (211 kB)\n",
      "     ---------------------------------------- 211.8/211.8 KB ? eta 0:00:00\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-11.3.0-cp39-cp39-win_amd64.whl (7.0 MB)\n",
      "     ---------------------------------------- 7.0/7.0 MB 44.7 MB/s eta 0:00:00\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.3.2-py3-none-any.whl (122 kB)\n",
      "     ---------------------------------------- 122.8/122.8 KB ? eta 0:00:00\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from importlib-resources>=5.12->nibabel->-r requirements.txt (line 2)) (3.23.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from pandas>=2.2.0->nilearn->-r requirements.txt (line 3)) (2025.3)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "     ------------------------------------- 509.2/509.2 KB 33.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from requests>=2.25.0->nilearn->-r requirements.txt (line 3)) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from requests>=2.25.0->nilearn->-r requirements.txt (line 3)) (2026.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from requests>=2.25.0->nilearn->-r requirements.txt (line 3)) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from requests>=2.25.0->nilearn->-r requirements.txt (line 3)) (3.11)\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ------------------------------------- 536.2/536.2 KB 35.1 MB/s eta 0:00:00\n",
      "Installing collected packages: pytz, mpmath, transforms3d, threadpoolctl, sympy, scipy, pyparsing, pillow, lxml, kiwisolver, joblib, importlib-resources, fonttools, cycler, contourpy, scikit-learn, pandas, nibabel, matplotlib, nipy, nilearn\n",
      "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.60.2 importlib-resources-6.5.2 joblib-1.5.3 kiwisolver-1.4.7 lxml-6.0.2 matplotlib-3.9.4 mpmath-1.3.0 nibabel-5.3.3 nilearn-0.12.1 nipy-0.6.1 pandas-2.3.3 pillow-11.3.0 pyparsing-3.3.2 pytz-2025.2 scikit-learn-1.6.1 scipy-1.13.1 sympy-1.14.0 threadpoolctl-3.6.0 transforms3d-0.4.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 25.3 is available.\n",
      "You should consider upgrading via the 'D:\\Side_Projects\\MRI_Project\\env_mri\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d10f690-6403-4cbf-aafb-5643e00cb99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import SimpleITK as sitk\n",
    "import psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4645a60-f948-4739-9b64-83258cbe4178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length patients 14\n"
     ]
    }
   ],
   "source": [
    "class patient:\n",
    "    '''\n",
    "    Struct for holding patient information and scan data\n",
    "    '''\n",
    "    def __init__(self,path):\n",
    "        self.info = {} #data-metadata pairs using pre-extension name\n",
    "        self.folder_path = path\n",
    "        self.date_loaded = time.time()\n",
    "        self.parse_and_assign_filenames(self.folder_path)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'{len(self.info.keys())} scans from {self.folder_path}'\n",
    "        \n",
    "    def parse_and_assign_filenames(self,path):\n",
    "        patient_scans=[]\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            # compressed_files = [file for file in files if file.split('.')[-2] == 'nii' and file.split('.')[-1] == 'gz']\n",
    "            compressed_files = [file for file in files if file.split('.')[-1] == 'gz']\n",
    "            for file in compressed_files:\n",
    "                self.info[file.split('.')[0]] = {\n",
    "                    'scan':os.path.join(root,file),\n",
    "                    'metadata':os.path.join(root,file.split('.')[0]+'.json') if os.path.exists(os.path.join(root,file.split('.')[0]+'.json')) else None,\n",
    "                }\n",
    "        \n",
    "    def load(self):\n",
    "        #return 4D set of values [(H,W,Scans(Depth),N),metadata]\n",
    "        def load_json(path):\n",
    "            with open(path) as f:\n",
    "                out = json.load(f)\n",
    "            return out\n",
    "        def load_scan(path):\n",
    "            # replace with datalad\n",
    "            img = nib.load(path)\n",
    "            data = np.asarray(img.dataobj)\n",
    "            return sitk.GetImageFromArray(data)\n",
    "        return {\n",
    "            'data':[load_scan(v['scan']) for k,v in self.info.items()], \n",
    "            'metadata':[[k,load_json(v['metadata'])] for k,v in self.info.items()]\n",
    "            }\n",
    "    def unload(self):\n",
    "        #use datalad to unload scan\n",
    "        pass\n",
    "class patient_dataset:\n",
    "    '''\n",
    "    Responsible for organizing and grouping scans + metadata per patient\n",
    "    Passes path to patient class \n",
    "    Also responsible for image preprocessing methods\n",
    "    '''\n",
    "    def __init__(self,path,standard_size=(256,256,200)):\n",
    "        #where path is the path to the dataset (should end in ds007045 or similar)\n",
    "        self.path = path\n",
    "        self.standard_size = standard_size\n",
    "        self.patients = []\n",
    "        for folder in os.listdir(self.path):\n",
    "            if self._is_folder(folder) == False:\n",
    "                continue\n",
    "            p = patient(os.path.join(self.path,folder))\n",
    "            if len(p.info) != 0: #filter non-patient folders\n",
    "                self.patients.append(p)\n",
    "        print('length patients', len(self.patients))\n",
    "        self.length = len(self.patients)\n",
    "    \n",
    "    def _is_folder(self,folder):\n",
    "        is_folder = True\n",
    "        if 'sub' not in folder.split('-'): #temp fix for picking up non-patient folders\n",
    "            is_folder = False\n",
    "        if os.path.isdir(os.path.join(self.path,folder)) == False:\n",
    "            is_folder = False\n",
    "        return is_folder\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Stream samples one-by-one without holding everything in memory.\n",
    "        \"\"\"\n",
    "        for file_id in range(self.length):\n",
    "            yield self.get(file_id)\n",
    "    \n",
    "    def __getitem__(self, file_id):\n",
    "        if isinstance(file_id, slice):\n",
    "            start, stop, step = file_id.indices(self.length)\n",
    "            return [self.get(i) for i in range(start, stop, step)]\n",
    "        elif isinstance(file_id, list):\n",
    "            return [self.get(i) for i in file_id]\n",
    "        elif isinstance(file_id, int):\n",
    "            if file_id < 0 or file_id >= self.length:\n",
    "                raise IndexError(\"patient index out of range\")\n",
    "            return self.get(file_id)\n",
    "        else:\n",
    "            raise TypeError(\"Indices must be integers, slices, or a list\")\n",
    "    \n",
    "    def get(self,file_id):\n",
    "        return self.patients[file_id].load()\n",
    "    \n",
    "    def sample(self):\n",
    "        #get one random patient obj and call get method\n",
    "        random_idx = random.randint(0,self.length)\n",
    "        return self.get(random_idx)\n",
    "    \n",
    "    def resample_to_shape(\n",
    "        self,\n",
    "        images, #list of sitk images\n",
    "        out_size,\n",
    "        interpolator=sitk.sitkLinear\n",
    "    ):\n",
    "        resampled_images = []\n",
    "        for img in images:\n",
    "            original_size = img.GetSize()\n",
    "            original_spacing = [1.0,1.0,1.0] #change to grabbing this from metadata\n",
    "            # original_spacing = self. #change to grabbing this from metadata\n",
    "        \n",
    "            new_spacing = [\n",
    "                (original_size[i] * original_spacing[i]) / out_size[i]\n",
    "                for i in range(3)\n",
    "            ]\n",
    "            \n",
    "            resampler = sitk.ResampleImageFilter()\n",
    "            \n",
    "            resampler.SetSize(out_size)\n",
    "            resampler.SetOutputSpacing(new_spacing)\n",
    "            resampler.SetInterpolator(interpolator)\n",
    "            resampler.SetOutputDirection(img.GetDirection())\n",
    "            resampler.SetOutputOrigin(img.GetOrigin())\n",
    "            resampled_images.append(resampler.Execute(img))\n",
    "        return resampled_images\n",
    "    \n",
    "    def preprocess(self,idx,count):\n",
    "        #standardize size\n",
    "        scan_sets = self.patients[idx:idx+count]\n",
    "        patient_scan_sets = [p['data'] for p in scan_sets]\n",
    "        resized_patient_scans = [self.resample_to_shape(patient_scans,self.standard_size) for patient_scans in patient_scan_sets]\n",
    "    \n",
    "    def generate_folds(self,k=10):\n",
    "        #Create an array from 0 to self.length, shuffle, and make k-1 even cuts \n",
    "        assignments = [i for i in range(self.length)]\n",
    "        random.shuffle(assignments)\n",
    "        fold_size = self.length//k #last fold will have extra items from excluded by rounding\n",
    "        self.folds = {}\n",
    "        for foldnum in range(k-2):\n",
    "            self.folds[foldnum] = assignments[fold_size*foldnum:fold_size*(foldnum+1)]\n",
    "        self.folds[k-1] = assignments[fold_size*(foldnum+1):]\n",
    "\n",
    "    def get_fold(self,fold_num):\n",
    "        assert len(self.folds.keys()) > 0\n",
    "        return self.__getitem__(self.folds[fold_num])#what if this ALSO returned a generator??\n",
    "# dataset = patient_dataset('ds007045')\n",
    "dataset = patient_dataset('ds007156')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "203380e6-f672-4620-a4e9-bdef0e7c1c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 'Minutes for ', 0, ' Scans')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "dataset.generate_folds(15)\n",
    "fold = dataset.get_fold(2)\n",
    "end = time.time()\n",
    "(end-start)/60, \"Minutes for \",len(fold),\" Scans\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "668f3ff9-a5fd-4827-a249-a2d40307d4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e9d7f15-3f47-4c51-bb30-a8612360bd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<SimpleITK.SimpleITK.Image; proxy of <Swig Object of type 'itk::simple::Image *' at 0x000001E2A601B990> >,\n",
       " <SimpleITK.SimpleITK.Image; proxy of <Swig Object of type 'itk::simple::Image *' at 0x000001E2A601B300> >,\n",
       " <SimpleITK.SimpleITK.Image; proxy of <Swig Object of type 'itk::simple::Image *' at 0x000001E2A601BF60> >,\n",
       " <SimpleITK.SimpleITK.Image; proxy of <Swig Object of type 'itk::simple::Image *' at 0x000001E2A601B960> >]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get(10)['data']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e3dc294-1dbf-4f47-ba52-f0be5d2c3095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.7974769433339435, 'Minutes for ', 337, ' images')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "print(len([d['data'] for d in dataset[0:100]]))\n",
    "end = time.time()\n",
    "(end-start)/60,'Minutes for ',dataset.length,' images' #2.7min for scans and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da36ad1e-20eb-44cb-9d34-f2b788495934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeba005-a4c6-4575-8f76-589fe5a28205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b63cefb-d3d8-4fef-b1a8-f8a432db1bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00423a83-6309-4bf2-95f9-b076e91df833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55199af6-29b8-4bc6-83a6-7b8a99edba8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d10a18-6d54-40c4-b2c1-2652ee141278",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
