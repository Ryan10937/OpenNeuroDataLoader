{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18c1c0e-9741-4206-a782-b5f2114bb937",
   "metadata": {},
   "source": [
    "# OpenNeuro Data Loader\n",
    "A data loader for open neuro MRI datasets https://openneuro.org/\n",
    "\n",
    "Getting usable data from open neuro was more difficult than it should be. I aim to create a 3 part system to expedite this process.\n",
    "\n",
    "The architecture is as follows:\n",
    "1. Given a dataset ID (ds#######) download the dataset to a specified folder and extract it using datalad\n",
    "1. A 'patient' class to hold data relevant to model training as well as data related to the patient\n",
    "1. A dataset class that has various dataset-related methods (preprocessing, train-val-test splits or stratified k-fold cross validation, ect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6701adc0-67ac-4d5f-9235-7b8a8f3600b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Todos\n",
    "1. Using datalad and git, download dataset\n",
    "1. Figure out memory measuring tool\n",
    "1. Load batch of n scans based on available memory\n",
    "1. Create generator of m batches of n scans which load on demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b84ed6-7239-43c8-9bb2-639e6d4d4a09",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95005cd-94a1-4afb-9a31-6ebb1edf4356",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nipy in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: nibabel in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 2)) (5.3.3)\n",
      "Requirement already satisfied: nilearn in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: numpy in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
      "Requirement already satisfied: scipy in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 5)) (1.13.1)\n",
      "Requirement already satisfied: matplotlib in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 6)) (3.9.4)\n",
      "Requirement already satisfied: SimpleITK in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 7)) (2.5.3)\n",
      "Requirement already satisfied: psutil in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 8)) (7.2.1)\n",
      "Requirement already satisfied: GitPython in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 9)) (3.1.46)\n",
      "Requirement already satisfied: sympy>=1.9 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nipy->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: transforms3d in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nipy->-r requirements.txt (line 1)) (0.4.2)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nibabel->-r requirements.txt (line 2)) (6.5.2)\n",
      "Requirement already satisfied: packaging>=20 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nibabel->-r requirements.txt (line 2)) (26.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nibabel->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: lxml in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nilearn->-r requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nilearn->-r requirements.txt (line 3)) (1.5.3)\n",
      "Requirement already satisfied: pandas>=2.2.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nilearn->-r requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: scikit-learn>=1.4.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nilearn->-r requirements.txt (line 3)) (1.6.1)\n",
      "Requirement already satisfied: requests>=2.25.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nilearn->-r requirements.txt (line 3)) (2.32.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (4.60.2)\n",
      "Requirement already satisfied: pillow>=8 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (11.3.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (3.3.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from GitPython->-r requirements.txt (line 9)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython->-r requirements.txt (line 9)) (5.0.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from importlib-resources>=5.12->nibabel->-r requirements.txt (line 2)) (3.23.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from pandas>=2.2.0->nilearn->-r requirements.txt (line 3)) (2025.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from pandas>=2.2.0->nilearn->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from requests>=2.25.0->nilearn->-r requirements.txt (line 3)) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from requests>=2.25.0->nilearn->-r requirements.txt (line 3)) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from requests>=2.25.0->nilearn->-r requirements.txt (line 3)) (2026.1.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from requests>=2.25.0->nilearn->-r requirements.txt (line 3)) (3.4.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from scikit-learn>=1.4.0->nilearn->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from sympy>=1.9->nipy->-r requirements.txt (line 1)) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 26.0.1 is available.\n",
      "You should consider upgrading via the 'D:\\Side_Projects\\MRI_Project\\env_mri\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d10f690-6403-4cbf-aafb-5643e00cb99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import SimpleITK as sitk\n",
    "import psutil\n",
    "# from datalad.api import get, drop\n",
    "import datalad.api as dl\n",
    "import shutil\n",
    "from datalad.api import install\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4645a60-f948-4739-9b64-83258cbe4178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ git-annex init\n",
      "$ git annex enableremote s3-PUBLIC\n",
      "length patients 79\n"
     ]
    }
   ],
   "source": [
    "class patient:\n",
    "    '''\n",
    "    Struct for holding patient information and scan data\n",
    "    '''\n",
    "    def __init__(self,path):\n",
    "        self.info = {} #data-metadata pairs using pre-extension name\n",
    "        self.folder_path = path\n",
    "        self.date_loaded = time.time()\n",
    "        self.parse_and_assign_filenames(self.folder_path)\n",
    "        self.loaded_onto_disk = False\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'{len(self.info.keys())} scans from {self.folder_path}'\n",
    "        \n",
    "    def parse_and_assign_filenames(self,path):\n",
    "        patient_scans=[]\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            # compressed_files = [file for file in files if file.split('.')[-2] == 'nii' and file.split('.')[-1] == 'gz']\n",
    "            compressed_files = [file for file in files if file.split('.')[-1] == 'gz']\n",
    "            for file in compressed_files:\n",
    "                self.info[file.split('.')[0]] = {\n",
    "                    'scan':os.path.join(root,file),\n",
    "                    'metadata':os.path.join(root,file.split('.')[0]+'.json') if os.path.exists(os.path.join(root,file.split('.')[0]+'.json')) else None,\n",
    "                }\n",
    "\n",
    "    \n",
    "    def load(self):\n",
    "        #return 4D set of values [(H,W,Scans(Depth),N),metadata]\n",
    "        def load_json(path:str):\n",
    "            if path==None or path=='':\n",
    "                return None\n",
    "            with open(path) as f:\n",
    "                out = json.load(f)\n",
    "            return out\n",
    "        def load_scan(path):\n",
    "            img = nib.load(path)\n",
    "            data = np.asarray(img.dataobj)\n",
    "            return sitk.GetImageFromArray(data)\n",
    "\n",
    "        #use datalad to fetch unavailable data\n",
    "        if self.loaded_onto_disk == False:\n",
    "            # for k,v in self.info.items():\n",
    "            #     dl.get(v['scan'])\n",
    "            dl.get(self.folder_path,jobs=4)\n",
    "            self.loaded_onto_disk = True\n",
    "\n",
    "        for k,v in self.info.items():\n",
    "            v['data'] = load_scan(v['scan'])\n",
    "            v['metadata_loaded'] = load_scan(v['scan'])\n",
    "        \n",
    "    def unload(self):\n",
    "        if self.loaded_onto_disk == False:\n",
    "            print(\"Unloading data that was never loaded...strange\")\n",
    "        #use datalad to unload scan\n",
    "        self.loaded_onto_disk = False\n",
    "        dl.drop(self.folder_path,recursive=True)\n",
    "\n",
    "    # def unload_from_ram(self):\n",
    "    #     if self.info['data'] == None or self.info['metadata_loaded'] == None:\n",
    "    #         print(\"Unloading data from RAM that was never loaded...strange\")\n",
    "    #     self.info['data'] = None \n",
    "    #     self.info['metadata_loaded'] = None\n",
    "        \n",
    "class patient_dataset:\n",
    "    '''\n",
    "    Responsible for organizing and grouping scans + metadata per patient\n",
    "    Passes path to patient class \n",
    "    Also responsible for image preprocessing methods\n",
    "    '''\n",
    "    def __init__(self,path,standard_size=(256,256,200)):\n",
    "        #where path is the path to the dataset (should end in ds007045 or similar)\n",
    "        self.ds = dl.install(\n",
    "            path=path,\n",
    "            source=f\"https://github.com/OpenNeuroDatasets/{path}.git\"\n",
    "        )\n",
    "        assert ds.is_installed()\n",
    "        self.run([\"git-annex\", \"init\"],dataset_path=path)\n",
    "        self.run([\"git\", \"annex\", \"enableremote\", \"s3-PUBLIC\"],dataset_path=path)\n",
    "        \n",
    "        self.path = path\n",
    "        self.standard_size = standard_size\n",
    "        self.patients = []\n",
    "        for folder in os.listdir(self.path):\n",
    "            if self._is_folder(folder) == False:\n",
    "                continue\n",
    "            p = patient(os.path.join(self.path,folder))\n",
    "            if len(p.info) != 0: #filter non-patient folders\n",
    "                self.patients.append(p)\n",
    "        print('length patients', len(self.patients))\n",
    "        self.length = len(self.patients)\n",
    "        self.loaded_idxs = []#if slow, replace with a deque\n",
    "        self.ram_loaded_idxs = []#if slow, replace with a deque\n",
    "        \n",
    "    def run(self,cmd, check=True,dataset_path=''):\n",
    "            print(f\"$ {' '.join(cmd)}\")\n",
    "            return subprocess.run(' '.join(cmd),cwd=dataset_path, check=False, capture_output=True)\n",
    "        \n",
    "    def _is_folder(self,folder):\n",
    "        is_folder = True\n",
    "        if 'sub' not in folder.split('-'): #temp fix for picking up non-patient folders\n",
    "            is_folder = False\n",
    "        if os.path.isdir(os.path.join(self.path,folder)) == False:\n",
    "            is_folder = False\n",
    "        return is_folder\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Stream samples one-by-one without holding everything in memory.\n",
    "        \"\"\"\n",
    "        for file_id in range(self.length):\n",
    "            yield self.get(file_id)\n",
    "    \n",
    "    def __getitem__(self, file_id):\n",
    "        print('here in getitem')\n",
    "        if isinstance(file_id, slice):\n",
    "            start, stop, step = file_id.indices(self.length)\n",
    "            return [self.get(i) for i in range(start, stop, step)]\n",
    "        elif isinstance(file_id, list):\n",
    "            return [self.get(i) for i in file_id]\n",
    "        elif isinstance(file_id, int):\n",
    "            if file_id < 0 or file_id >= self.length:\n",
    "                raise IndexError(\"patient index out of range\")\n",
    "            return self.get(file_id)\n",
    "        else:\n",
    "            raise TypeError(\"Indices must be integers, slices, or a list\")\n",
    "    \n",
    "    def get(self,file_id):#wilo 2/16 6pm, test the item loading and dropping. Consider replacing the \"what to drop\" with the datalad process\n",
    "        print('here in get')\n",
    "        #check available memory\n",
    "        # available_ram,total_ram,percent_ram_used = self.get_ram_info()\n",
    "        available_disk,total_disk,percent_disk_used = self.get_disk_info('D:\\\\')#hardcoded disk\n",
    "        available_disk = 10e+9 #10GB\n",
    "        file_stats = ds.status(#wilo use this to detemine if we should go get other data\n",
    "            # path=\"sub-01/ses-T1\",\n",
    "            path=self.folder_path,\n",
    "            annex=\"all\",\n",
    "            recursive=True,\n",
    "            return_type=\"list\",\n",
    "        )\n",
    "        patient_folder_size = [r[\"bytesize\"] for r in ds.status(path=self.folder_path,annex=\"all\",recursive=True,return_type=\"list\") if \"bytesize\" in r]\n",
    "        \n",
    "        #This will fail with OOM if the file size is more than 10% of RAM or disk space\n",
    "        while available_disk < patient_folder_size :\n",
    "            self.drop_an_item()\n",
    "            patient_folder_size =[r[\"bytesize\"] for r in ds.status(path=self.folder_path,annex=\"all\",recursive=True,return_type=\"list\") if \"bytesize\" in r]\n",
    "        self.loaded_idxs.append(file_id)\n",
    "        \n",
    "        # while percent_ram_used > 0.9 : #this is imperfect and should check how large the incoming data is. \n",
    "        #     self.patients[self.ram_loaded_idxs[0]].unload_from_ram()\n",
    "        #     self.ram_loaded_idxs.pop(0)\n",
    "        #     print('popping item from working memory')\n",
    "        #     available_ram,total_ram,percent_ram_used = self.get_ram_info()\n",
    "        # self.ram_loaded_idxs.append(file_id)\n",
    "        self.patients[file_id].load()\n",
    "        return self.patients[file_id].info\n",
    "        \n",
    "    def drop_an_item(self):\n",
    "        '''\n",
    "        Drop an item from disk using datalad\n",
    "        '''\n",
    "        print('dropping file with ID:',self.loaded_idxs[0])\n",
    "        self.patients[self.loaded_idxs[0]].unload()\n",
    "        self.loaded_idxs.pop(0)#if slow, replace with a deque\n",
    "        \n",
    "    def get_ram_info(self):\n",
    "        vm = psutil.virtual_memory()\n",
    "        total_ram = vm.total      # bytes\n",
    "        available_ram = vm.available  # bytes\n",
    "        return available_ram, total_ram, available_ram/total_ram\n",
    "\n",
    "    def get_disk_info(self, path=\"/\"):\n",
    "        usage = shutil.disk_usage(path)\n",
    "        total = usage.total      # bytes\n",
    "        available = usage.free   # bytes\n",
    "        return available, total, available/total\n",
    "\n",
    "    def sample(self):\n",
    "        #get one random patient obj and call get method\n",
    "        random_idx = random.randint(0,self.length)\n",
    "        return self.get(random_idx)\n",
    "        \n",
    "    def resample_to_shape(\n",
    "        self,\n",
    "        images, #list of sitk images\n",
    "        out_size,\n",
    "        interpolator=sitk.sitkLinear\n",
    "    ):\n",
    "        resampled_images = []\n",
    "        for img in images:\n",
    "            original_size = img.GetSize()\n",
    "            original_spacing = [1.0,1.0,1.0] #change to grabbing this from metadata\n",
    "            # original_spacing = self. #change to grabbing this from metadata\n",
    "        \n",
    "            new_spacing = [\n",
    "                (original_size[i] * original_spacing[i]) / out_size[i]\n",
    "                for i in range(3)\n",
    "            ]\n",
    "            \n",
    "            resampler = sitk.ResampleImageFilter()\n",
    "            \n",
    "            resampler.SetSize(out_size)\n",
    "            resampler.SetOutputSpacing(new_spacing)\n",
    "            resampler.SetInterpolator(interpolator)\n",
    "            resampler.SetOutputDirection(img.GetDirection())\n",
    "            resampler.SetOutputOrigin(img.GetOrigin())\n",
    "            resampled_images.append(resampler.Execute(img))\n",
    "        return resampled_images\n",
    "    \n",
    "    def preprocess(self,idx,count):\n",
    "        #standardize size\n",
    "        scan_sets = self.patients[idx:idx+count]\n",
    "        patient_scan_sets = [p['data'] for p in scan_sets]\n",
    "        resized_patient_scans = [self.resample_to_shape(patient_scans,self.standard_size) for patient_scans in patient_scan_sets]\n",
    "    \n",
    "    def generate_folds(self,k=10):\n",
    "        #Create an array from 0 to self.length, shuffle, and make k-1 even cuts \n",
    "        assignments = [i for i in range(self.length)]\n",
    "        random.shuffle(assignments)\n",
    "        fold_size = self.length//k #last fold will have extra items from excluded by rounding\n",
    "        self.folds = {}\n",
    "        for foldnum in range(k-2):\n",
    "            self.folds[foldnum] = assignments[fold_size*foldnum:fold_size*(foldnum+1)]\n",
    "        self.folds[k-1] = assignments[fold_size*(foldnum+1):]\n",
    "\n",
    "    def get_fold(self,fold_num):\n",
    "        assert len(self.folds.keys()) > 0\n",
    "        return self.__getitem__(self.folds[fold_num])#what if this ALSO returned a generator??\n",
    "# dataset = patient_dataset('ds007045')\n",
    "# dataset = patient_dataset('ds007156')\n",
    "dataset = patient_dataset('ds002424')\n",
    "# dataset = patient_dataset('ds002424',gb_limit=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "203380e6-f672-4620-a4e9-bdef0e7c1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dataset.generate_folds(20)\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba9ee520-3735-4a35-9d40-28c183e30110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here in getitem\n",
      "here in get\n",
      "here in get\n",
      "here in get\n",
      "here in getitem\n",
      "here in get\n",
      "here in get\n",
      "here in get\n",
      "here in getitem\n",
      "here in get\n",
      "here in get\n",
      "here in get\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.644381908575694, 'Minutes for ', 3, ' Patients')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "fold = dataset.get_fold(1)\n",
    "fold = dataset.get_fold(2)\n",
    "fold = dataset.get_fold(3)\n",
    "end = time.time()\n",
    "(end-start)/60, \"Minutes for \",len(fold),\" Patients\" #2.7min per patient (9 scans) for scans and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "668f3ff9-a5fd-4827-a249-a2d40307d4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14360"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pympler import asizeof\n",
    "asizeof.asizeof(fold) #23680"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f0e481a-798f-4369-acb7-077067aa491b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "244d4577-26ab-418e-b72b-8f07c70030fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51a8cd-363e-4adb-8ca7-bfbf3b18862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wilo 2/9: I have to rethink how im keeping/dropping items for RAM/disk\n",
    "#if the dataset is too large for the disk, its certainly too large for RAM. \n",
    "#so i definitely have to manage both.\n",
    "#i should NOT rely on fold size for memory management. That is bad\n",
    "#i should not wait until model-run time to dl get\n",
    "\n",
    "#what if i worked on a batching system that was unrelated to the folds\n",
    "#it would be assumed that you have 2x disc space than your batch size and x RAM to your batch size\n",
    "#and while one batch was being used, another is getting downloaded with datalad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd09b94-750c-42de-99e5-162ec14f517e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c0782-ccf3-42d8-a29f-97bcb0f11fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The problem statement is: Make a dataloader for OpenNeuro where the dataset is larger than available disk (and RAM) space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2630dde9-b0bc-4913-a1ee-296e85302d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pympler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716c031-2d11-475d-b3c4-d57b8788dc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c70ff-be98-4b5d-83f9-b54e7a5a03b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808f310-2dca-48fc-9d1b-dc49f29644bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e96d96f0-5276-46d4-8889-a02d1bc0fff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "untracked: sub-03\\ses-T1\\func\\.ipynb_checkpoints (directory)\n",
      "638 annex'd files (39.9 GB/40.5 GB present/total size)\n",
      "Total MB: 43436.320562\n"
     ]
    }
   ],
   "source": [
    "from datalad.api import Dataset\n",
    "\n",
    "ds = Dataset(\"ds002424\")\n",
    "\n",
    "results = ds.status(#wilo use this to detemine if we should go get other data\n",
    "    # path=\"sub-01/ses-T1\",\n",
    "    path=\"\",\n",
    "    annex=\"all\",\n",
    "    recursive=True,\n",
    "    return_type=\"list\",\n",
    ")\n",
    "\n",
    "total_bytes = 0\n",
    "\n",
    "for r in results:\n",
    "    if \"bytesize\" in r:\n",
    "        total_bytes += r[\"bytesize\"]\n",
    "        # print(r[\"path\"], r[\"bytesize\"], r.get(\"has_content\"))\n",
    "\n",
    "print(\"Total MB:\", total_bytes/1e+6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4582ff1e-4bbb-40ad-99b4-8555b586ceed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "913ea0e9-da67-4b02-8796-b4b2a27617a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'file', 'gitshasum': '9ac951ec58d53650cbe7887b623fe18aadf09b52', 'bytesize': 5025, 'prev_gitshasum': '9ac951ec58d53650cbe7887b623fe18aadf09b52', 'state': 'clean', 'path': 'D:\\\\Side_Projects\\\\MRI_Project\\\\OpenNeuroDataLoader\\\\ds002424\\\\sub-01\\\\ses-T1\\\\func\\\\sub-01_ses-T1_task-SSI_events.tsv', 'parentds': 'D:\\\\Side_Projects\\\\MRI_Project\\\\OpenNeuroDataLoader\\\\ds002424', 'status': 'ok', 'refds': 'D:\\\\Side_Projects\\\\MRI_Project\\\\OpenNeuroDataLoader\\\\ds002424', 'action': 'status'}\n"
     ]
    }
   ],
   "source": [
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dbf754b0-81bb-48ef-ab9a-7943e64bb007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m\n",
       "\u001b[0mds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mannex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0muntracked\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'normal'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrecursive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mrecursion_limit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0meval_subdataset_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'full'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mreport_filetype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Report on the state of dataset content.\n",
       "\n",
       "This is an analog to `git status` that is simultaneously crippled and more\n",
       "powerful. It is crippled, because it only supports a fraction of the\n",
       "functionality of its counter part and only distinguishes a subset of the\n",
       "states that Git knows about. But it is also more powerful as it can handle\n",
       "status reports for a whole hierarchy of datasets, with the ability to\n",
       "report on a subset of the content (selection of paths) across any number\n",
       "of datasets in the hierarchy.\n",
       "\n",
       "*Path conventions*\n",
       "\n",
       "All reports are guaranteed to use absolute paths that are underneath the\n",
       "given or detected reference dataset, regardless of whether query paths are\n",
       "given as absolute or relative paths (with respect to the working directory,\n",
       "or to the reference dataset, when such a dataset is given explicitly).\n",
       "Moreover, so-called \"explicit relative paths\" (i.e. paths that start with\n",
       "'.' or '..') are also supported, and are interpreted as relative paths with\n",
       "respect to the current working directory regardless of whether a reference\n",
       "dataset with specified.\n",
       "\n",
       "When it is necessary to address a subdataset record in a superdataset\n",
       "without causing a status query for the state _within_ the subdataset\n",
       "itself, this can be achieved by explicitly providing a reference dataset\n",
       "and the path to the root of the subdataset like so::\n",
       "\n",
       "  datalad status --dataset . subdspath\n",
       "\n",
       "In contrast, when the state of the subdataset within the superdataset is\n",
       "not relevant, a status query for the content of the subdataset can be\n",
       "obtained by adding a trailing path separator to the query path (rsync-like\n",
       "syntax)::\n",
       "\n",
       "  datalad status --dataset . subdspath/\n",
       "\n",
       "When both aspects are relevant (the state of the subdataset content\n",
       "and the state of the subdataset within the superdataset), both queries\n",
       "can be combined::\n",
       "\n",
       "  datalad status --dataset . subdspath subdspath/\n",
       "\n",
       "When performing a recursive status query, both status aspects of subdataset\n",
       "are always included in the report.\n",
       "\n",
       "\n",
       "*Content types*\n",
       "\n",
       "The following content types are distinguished:\n",
       "\n",
       "- 'dataset' -- any top-level dataset, or any subdataset that is properly\n",
       "  registered in superdataset\n",
       "- 'directory' -- any directory that does not qualify for type 'dataset'\n",
       "- 'file' -- any file, or any symlink that is placeholder to an annexed\n",
       "  file when annex-status reporting is enabled\n",
       "- 'symlink' -- any symlink that is not used as a placeholder for an annexed\n",
       "  file\n",
       "\n",
       "*Content states*\n",
       "\n",
       "The following content states are distinguished:\n",
       "\n",
       "- 'clean'\n",
       "- 'added'\n",
       "- 'modified'\n",
       "- 'deleted'\n",
       "- 'untracked'\n",
       "\n",
       "Examples\n",
       "--------\n",
       "Report on the state of a dataset::\n",
       "\n",
       "   > status()\n",
       "\n",
       "Report on the state of a dataset and all subdatasets::\n",
       "\n",
       "   > status(recursive=True)\n",
       "\n",
       "Address a subdataset record in a superdataset without causing a status\n",
       "query for the state _within_ the subdataset itself::\n",
       "\n",
       "   > status(dataset='.', path='mysubdataset')\n",
       "\n",
       "Get a status query for the state within the subdataset without causing\n",
       "a status query for the superdataset (using trailing path separator in\n",
       "the query path):::\n",
       "\n",
       "   > status(dataset='.', path='mysubdataset/')\n",
       "\n",
       "Report on the state of a subdataset in a superdataset and on the state\n",
       "within the subdataset::\n",
       "\n",
       "   > status(dataset='.', path=['mysubdataset', 'mysubdataset/'])\n",
       "\n",
       "Report the file size of annexed content in a dataset::\n",
       "\n",
       "   > status(annex=True)\n",
       "\n",
       "\n",
       "Parameters\n",
       "----------\n",
       "path : sequence of str or None, optional\n",
       "  path to be evaluated. [Default: None]\n",
       "dataset : Dataset or None, optional\n",
       "  specify the dataset to query.  If no dataset is given, an attempt is\n",
       "  made to identify the dataset based on the current working directory.\n",
       "  [Default: None]\n",
       "annex : {None, 'basic', 'availability', 'all'}, optional\n",
       "  Switch whether to include information on the annex content of\n",
       "  individual files in the status report, such as recorded file size.\n",
       "  By default no annex information is reported (faster). Three report\n",
       "  modes are available: basic information like file size and key name\n",
       "  ('basic'); additionally test whether file content is present in the\n",
       "  local annex ('availability'; requires one or two additional file\n",
       "  system stat calls, but does not call git-annex), this will add the\n",
       "  result properties 'has_content' (boolean flag) and 'objloc'\n",
       "  (absolute path to an existing annex object file); or 'all' which\n",
       "  will report all available information (presently identical to\n",
       "  'availability'). [Default: None]\n",
       "untracked : {'no', 'normal', 'all'}, optional\n",
       "  If and how untracked content is reported when comparing a revision\n",
       "  to the state of the working tree. 'no': no untracked content is\n",
       "  reported; 'normal': untracked files and entire untracked directories\n",
       "  are reported as such; 'all': report individual files even in fully\n",
       "  untracked directories. [Default: 'normal']\n",
       "recursive : bool, optional\n",
       "  if set, recurse into potential subdatasets. [Default: False]\n",
       "recursion_limit : int or None, optional\n",
       "  limit recursion into subdatasets to the given number of levels.\n",
       "  [Default: None]\n",
       "eval_subdataset_state : {'no', 'commit', 'full'}, optional\n",
       "  Evaluation of subdataset state (clean vs. modified) can be expensive\n",
       "  for deep dataset hierarchies as subdataset have to be tested\n",
       "  recursively for uncommitted modifications. Setting this option to\n",
       "  'no' or 'commit' can substantially boost performance by limiting\n",
       "  what is being tested. With 'no' no state is evaluated and subdataset\n",
       "  result records typically do not contain a 'state' property. With\n",
       "  'commit' only a discrepancy of the HEAD commit shasum of a\n",
       "  subdataset and the shasum recorded in the superdataset's record is\n",
       "  evaluated, and the 'state' result property only reflects this\n",
       "  aspect. With 'full' any other modification is considered too (see\n",
       "  the 'untracked' option for further tailoring modification testing).\n",
       "  [Default: 'full']\n",
       "report_filetype : {'raw', 'eval', None}, optional\n",
       "  THIS OPTION IS IGNORED. It will be removed in a future release.\n",
       "  Dataset component types are always reported as-is (previous 'raw'\n",
       "  mode), unless annex-reporting is enabled with the `annex` option, in\n",
       "  which case symlinks that represent annexed files will be reported as\n",
       "  type='file'. [Default: None]\n",
       "on_failure : {'ignore', 'continue', 'stop'}, optional\n",
       "  behavior to perform on failure: 'ignore' any failure is reported,\n",
       "  but does not cause an exception; 'continue' if any failure occurs an\n",
       "  exception will be raised at the end, but processing other actions\n",
       "  will continue for as long as possible; 'stop': processing will stop\n",
       "  on first failure and an exception is raised. A failure is any result\n",
       "  with status 'impossible' or 'error'. Raised exception is an\n",
       "  IncompleteResultsError that carries the result dictionaries of the\n",
       "  failures in its `failed` attribute. [Default: 'continue']\n",
       "result_filter : callable or None, optional\n",
       "  if given, each to-be-returned status dictionary is passed to this\n",
       "  callable, and is only returned if the callable's return value does\n",
       "  not evaluate to False or a ValueError exception is raised. If the\n",
       "  given callable supports `**kwargs` it will additionally be passed\n",
       "  the keyword arguments of the original API call. [Default: None]\n",
       "result_renderer\n",
       "  select rendering mode command results. 'tailored' enables a command-\n",
       "  specific rendering style that is typically tailored to human\n",
       "  consumption, if there is one for a specific command, or otherwise\n",
       "  falls back on the the 'generic' result renderer; 'generic' renders\n",
       "  each result in one line  with key info like action, status, path,\n",
       "  and an optional message); 'json' a complete JSON line serialization\n",
       "  of the full result record; 'json_pp' like 'json', but pretty-printed\n",
       "  spanning multiple lines; 'disabled' turns off result rendering\n",
       "  entirely; '<template>' reports any value(s) of any result properties\n",
       "  in any format indicated by the template (e.g. '{path}', compare with\n",
       "  JSON output for all key-value choices). The template syntax follows\n",
       "  the Python \"format() language\". It is possible to report individual\n",
       "  dictionary values, e.g. '{metadata[name]}'. If a 2nd-level key\n",
       "  contains a colon, e.g. 'music:Genre', ':' must be substituted by '#'\n",
       "  in the template, like so: '{metadata[music#Genre]}'. [Default:\n",
       "  'tailored']\n",
       "result_xfm : {'datasets', 'successdatasets-or-none', 'paths', 'relpaths', 'metadata'} or callable or None, optional\n",
       "  if given, each to-be-returned result status dictionary is passed to\n",
       "  this callable, and its return value becomes the result instead. This\n",
       "  is different from `result_filter`, as it can perform arbitrary\n",
       "  transformation of the result value. This is mostly useful for top-\n",
       "  level command invocations that need to provide the results in a\n",
       "  particular format. Instead of a callable, a label for a pre-crafted\n",
       "  result transformation can be given. [Default: None]\n",
       "return_type : {'generator', 'list', 'item-or-list'}, optional\n",
       "  return value behavior switch. If 'item-or-list' a single value is\n",
       "  returned instead of a one-item return value list, or a list in case\n",
       "  of multiple return values. `None` is return in case of an empty\n",
       "  list. [Default: 'list']\n",
       "\u001b[1;31mFile:\u001b[0m      d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages\\datalad\\core\\local\\status.py\n",
       "\u001b[1;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds.status?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591524a0-4b18-41ff-8a26-1b08dc20cb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
