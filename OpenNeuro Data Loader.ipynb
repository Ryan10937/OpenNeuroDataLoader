{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e18c1c0e-9741-4206-a782-b5f2114bb937",
   "metadata": {},
   "source": [
    "# OpenNeuro Data Loader\n",
    "A data loader for open neuro MRI datasets https://openneuro.org/\n",
    "\n",
    "Getting usable data from open neuro was more difficult than it should be. I aim to create a 3 part system to expedite this process.\n",
    "\n",
    "The architecture is as follows:\n",
    "1. Given a dataset ID (ds#######) download the dataset to a specified folder and extract it using datalad\n",
    "1. A 'patient' class to hold data relevant to model training as well as data related to the patient\n",
    "1. A dataset class that has various dataset-related methods (preprocessing, train-val-test splits or stratified k-fold cross validation, ect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6701adc0-67ac-4d5f-9235-7b8a8f3600b6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Todos\n",
    "1. Using datalad and git, download dataset\n",
    "1. Figure out memory measuring tool\n",
    "1. Load batch of n scans based on available memory\n",
    "1. Create generator of m batches of n scans which load on demand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b84ed6-7239-43c8-9bb2-639e6d4d4a09",
   "metadata": {},
   "source": [
    "## Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f95005cd-94a1-4afb-9a31-6ebb1edf4356",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: nipy in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 1)) (0.6.1)\n",
      "Requirement already satisfied: nibabel in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 2)) (5.3.3)\n",
      "Requirement already satisfied: nilearn in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 3)) (0.12.1)\n",
      "Requirement already satisfied: numpy in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 4)) (2.0.2)\n",
      "Requirement already satisfied: scipy in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 5)) (1.13.1)\n",
      "Requirement already satisfied: matplotlib in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 6)) (3.9.4)\n",
      "Requirement already satisfied: SimpleITK in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 7)) (2.5.3)\n",
      "Requirement already satisfied: psutil in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 8)) (7.2.1)\n",
      "Requirement already satisfied: GitPython in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from -r requirements.txt (line 9)) (3.1.46)\n",
      "Requirement already satisfied: transforms3d in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nipy->-r requirements.txt (line 1)) (0.4.2)\n",
      "Requirement already satisfied: sympy>=1.9 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nipy->-r requirements.txt (line 1)) (1.14.0)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nibabel->-r requirements.txt (line 2)) (6.5.2)\n",
      "Requirement already satisfied: packaging>=20 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nibabel->-r requirements.txt (line 2)) (26.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nibabel->-r requirements.txt (line 2)) (4.15.0)\n",
      "Requirement already satisfied: requests>=2.25.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nilearn->-r requirements.txt (line 3)) (2.32.5)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nilearn->-r requirements.txt (line 3)) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn>=1.4.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nilearn->-r requirements.txt (line 3)) (1.6.1)\n",
      "Requirement already satisfied: pandas>=2.2.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nilearn->-r requirements.txt (line 3)) (2.3.3)\n",
      "Requirement already satisfied: lxml in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from nilearn->-r requirements.txt (line 3)) (6.0.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (4.60.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.4.7)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (3.3.2)\n",
      "Requirement already satisfied: pillow>=8 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (11.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (0.12.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (1.3.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from matplotlib->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from GitPython->-r requirements.txt (line 9)) (4.0.12)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython->-r requirements.txt (line 9)) (5.0.2)\n",
      "Requirement already satisfied: zipp>=3.1.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from importlib-resources>=5.12->nibabel->-r requirements.txt (line 2)) (3.23.0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from pandas>=2.2.0->nilearn->-r requirements.txt (line 3)) (2025.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from pandas>=2.2.0->nilearn->-r requirements.txt (line 3)) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from requests>=2.25.0->nilearn->-r requirements.txt (line 3)) (3.11)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from requests>=2.25.0->nilearn->-r requirements.txt (line 3)) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from requests>=2.25.0->nilearn->-r requirements.txt (line 3)) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from requests>=2.25.0->nilearn->-r requirements.txt (line 3)) (2026.1.4)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from scikit-learn>=1.4.0->nilearn->-r requirements.txt (line 3)) (3.6.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\side_projects\\mri_project\\env_mri\\lib\\site-packages (from sympy>=1.9->nipy->-r requirements.txt (line 1)) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.4; however, version 26.0.1 is available.\n",
      "You should consider upgrading via the 'D:\\Side_Projects\\MRI_Project\\env_mri\\Scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d10f690-6403-4cbf-aafb-5643e00cb99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import SimpleITK as sitk\n",
    "import psutil\n",
    "# from datalad.api import get, drop\n",
    "import datalad.api as dl\n",
    "import shutil\n",
    "from datalad.api import install\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4645a60-f948-4739-9b64-83258cbe4178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$ git-annex init\n",
      "$ git annex enableremote s3-PUBLIC\n",
      "length patients 79\n"
     ]
    }
   ],
   "source": [
    "class patient:\n",
    "    '''\n",
    "    Struct for holding patient information and scan data\n",
    "    '''\n",
    "    def __init__(self,path):\n",
    "        self.info = {} #data-metadata pairs using pre-extension name\n",
    "        self.folder_path = path\n",
    "        self.date_loaded = time.time()\n",
    "        self.parse_and_assign_filenames(self.folder_path)\n",
    "        self.loaded_onto_disk = False\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'{len(self.info.keys())} scans from {self.folder_path}'\n",
    "        \n",
    "    def parse_and_assign_filenames(self,path):\n",
    "        patient_scans=[]\n",
    "        for root,dirs,files in os.walk(path):\n",
    "            # compressed_files = [file for file in files if file.split('.')[-2] == 'nii' and file.split('.')[-1] == 'gz']\n",
    "            compressed_files = [file for file in files if file.split('.')[-1] == 'gz']\n",
    "            for file in compressed_files:\n",
    "                self.info[file.split('.')[0]] = {\n",
    "                    'scan':os.path.join(root,file),\n",
    "                    'metadata':os.path.join(root,file.split('.')[0]+'.json') if os.path.exists(os.path.join(root,file.split('.')[0]+'.json')) else None,\n",
    "                }\n",
    "\n",
    "    \n",
    "    def load(self):\n",
    "        #return 4D set of values [(H,W,Scans(Depth),N),metadata]\n",
    "        def load_json(path:str):\n",
    "            if path==None or path=='':\n",
    "                return None\n",
    "            with open(path) as f:\n",
    "                out = json.load(f)\n",
    "            return out\n",
    "        def load_scan(path):\n",
    "            img = nib.load(path)\n",
    "            data = np.asarray(img.dataobj)\n",
    "            return sitk.GetImageFromArray(data)\n",
    "\n",
    "        #use datalad to fetch unavailable data\n",
    "        if self.loaded_onto_disk == False:\n",
    "            # for k,v in self.info.items():\n",
    "            #     dl.get(v['scan'])\n",
    "            dl.get(self.folder_path,jobs=8)\n",
    "            self.loaded_onto_disk = True\n",
    "\n",
    "        for k,v in self.info.items():\n",
    "            v['data'] = load_scan(v['scan'])\n",
    "            v['metadata_loaded'] = load_scan(v['scan'])\n",
    "        \n",
    "    def unload(self):\n",
    "        if self.loaded_onto_disk == False:\n",
    "            print(\"Unloading data that was never loaded...strange\")\n",
    "        #use datalad to unload scan\n",
    "        self.loaded_onto_disk = False\n",
    "        dl.drop(self.folder_path,recursive=True)\n",
    "\n",
    "    def unload_from_ram(self):\n",
    "        if self.info['data'] == None or self.info['metadata_loaded'] == None:\n",
    "            print(\"Unloading data from RAM that was never loaded...strange\")\n",
    "        self.info['data'] = None \n",
    "        self.info['metadata_loaded'] = None\n",
    "        \n",
    "class patient_dataset:\n",
    "    '''\n",
    "    Responsible for organizing and grouping scans + metadata per patient\n",
    "    Passes path to patient class \n",
    "    Also responsible for image preprocessing methods\n",
    "    '''\n",
    "    def __init__(self,path,standard_size=(256,256,200)):\n",
    "        #where path is the path to the dataset (should end in ds007045 or similar)\n",
    "        dl.install(\n",
    "            path=path,\n",
    "            source=f\"https://github.com/OpenNeuroDatasets/{path}.git\"\n",
    "        )\n",
    "        \n",
    "        self.run([\"git-annex\", \"init\"],dataset_path=path)\n",
    "        self.run([\"git\", \"annex\", \"enableremote\", \"s3-PUBLIC\"],dataset_path=path)\n",
    "        \n",
    "        self.path = path\n",
    "        self.standard_size = standard_size\n",
    "        self.patients = []\n",
    "        for folder in os.listdir(self.path):\n",
    "            if self._is_folder(folder) == False:\n",
    "                continue\n",
    "            p = patient(os.path.join(self.path,folder))\n",
    "            if len(p.info) != 0: #filter non-patient folders\n",
    "                self.patients.append(p)\n",
    "        print('length patients', len(self.patients))\n",
    "        self.length = len(self.patients)\n",
    "        self.loaded_idxs = []#if slow, replace with a deque\n",
    "        self.ram_loaded_idxs = []#if slow, replace with a deque\n",
    "        \n",
    "    def run(self,cmd, check=True,dataset_path=''):\n",
    "            print(f\"$ {' '.join(cmd)}\")\n",
    "            return subprocess.run(' '.join(cmd),cwd=dataset_path, check=False, capture_output=True)\n",
    "        \n",
    "    def _is_folder(self,folder):\n",
    "        is_folder = True\n",
    "        if 'sub' not in folder.split('-'): #temp fix for picking up non-patient folders\n",
    "            is_folder = False\n",
    "        if os.path.isdir(os.path.join(self.path,folder)) == False:\n",
    "            is_folder = False\n",
    "        return is_folder\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        Stream samples one-by-one without holding everything in memory.\n",
    "        \"\"\"\n",
    "        for file_id in range(self.length):\n",
    "            yield self.get(file_id)\n",
    "    \n",
    "    def __getitem__(self, file_id):\n",
    "        if isinstance(file_id, slice):\n",
    "            start, stop, step = file_id.indices(self.length)\n",
    "            return [self.get(i) for i in range(start, stop, step)]\n",
    "        elif isinstance(file_id, list):\n",
    "            return [self.get(i) for i in file_id]\n",
    "        elif isinstance(file_id, int):\n",
    "            if file_id < 0 or file_id >= self.length:\n",
    "                raise IndexError(\"patient index out of range\")\n",
    "            return self.get(file_id)\n",
    "        else:\n",
    "            raise TypeError(\"Indices must be integers, slices, or a list\")\n",
    "    \n",
    "    def get(self,file_id):\n",
    "        #check available memory\n",
    "        available_ram,total_ram,percent_ram_used = self.get_ram_info()\n",
    "        available_disk,total_disk,percent_disk_used = self.get_disk_info('D:\\\\')#hardcoded disk\n",
    "        \n",
    "        #This will fail with OOM if the file size is more than 10% of RAM or disk space\n",
    "        while percent_disk_used > 0.9: \n",
    "            self.drop_an_item()\n",
    "            available_disk,total_disk,percent_disk_used = self.get_disk_info('D:\\\\')#hardcoded disk\n",
    "        self.loaded_idxs.append(file_id)\n",
    "        \n",
    "        while percent_ram_used > 0.9 : #this is imperfect and should check how large the incoming data is. \n",
    "            self.patients[self.ram_loaded_idxs[0]].unload_from_ram()\n",
    "            self.ram_loaded_idxs.pop(0)\n",
    "            print('popping item from working memory')\n",
    "            available_ram,total_ram,percent_ram_used = self.get_ram_info()\n",
    "        self.ram_loaded_idxs.append(file_id)\n",
    "        return self.patients[file_id].load()\n",
    "        \n",
    "    def drop_an_item(self):\n",
    "        '''\n",
    "        Drop an item from disk using datalad\n",
    "        '''\n",
    "        print('dropping file with ID:',self.loaded_idxs[0])\n",
    "        self.patients[self.loaded_idxs[0]].unload()\n",
    "        self.loaded_idxs.pop(0)#if slow, replace with a deque\n",
    "        \n",
    "    def get_ram_info(self):\n",
    "        vm = psutil.virtual_memory()\n",
    "        total_ram = vm.total      # bytes\n",
    "        available_ram = vm.available  # bytes\n",
    "        return available_ram, total_ram, available_ram/total_ram\n",
    "\n",
    "    def get_disk_info(self, path=\"/\"):\n",
    "        usage = shutil.disk_usage(path)\n",
    "        total = usage.total      # bytes\n",
    "        available = usage.free   # bytes\n",
    "        return available, total, available/total\n",
    "\n",
    "    def sample(self):\n",
    "        #get one random patient obj and call get method\n",
    "        random_idx = random.randint(0,self.length)\n",
    "        return self.get(random_idx)\n",
    "        \n",
    "    def resample_to_shape(\n",
    "        self,\n",
    "        images, #list of sitk images\n",
    "        out_size,\n",
    "        interpolator=sitk.sitkLinear\n",
    "    ):\n",
    "        resampled_images = []\n",
    "        for img in images:\n",
    "            original_size = img.GetSize()\n",
    "            original_spacing = [1.0,1.0,1.0] #change to grabbing this from metadata\n",
    "            # original_spacing = self. #change to grabbing this from metadata\n",
    "        \n",
    "            new_spacing = [\n",
    "                (original_size[i] * original_spacing[i]) / out_size[i]\n",
    "                for i in range(3)\n",
    "            ]\n",
    "            \n",
    "            resampler = sitk.ResampleImageFilter()\n",
    "            \n",
    "            resampler.SetSize(out_size)\n",
    "            resampler.SetOutputSpacing(new_spacing)\n",
    "            resampler.SetInterpolator(interpolator)\n",
    "            resampler.SetOutputDirection(img.GetDirection())\n",
    "            resampler.SetOutputOrigin(img.GetOrigin())\n",
    "            resampled_images.append(resampler.Execute(img))\n",
    "        return resampled_images\n",
    "    \n",
    "    def preprocess(self,idx,count):\n",
    "        #standardize size\n",
    "        scan_sets = self.patients[idx:idx+count]\n",
    "        patient_scan_sets = [p['data'] for p in scan_sets]\n",
    "        resized_patient_scans = [self.resample_to_shape(patient_scans,self.standard_size) for patient_scans in patient_scan_sets]\n",
    "    \n",
    "    def generate_folds(self,k=10):\n",
    "        #Create an array from 0 to self.length, shuffle, and make k-1 even cuts \n",
    "        assignments = [i for i in range(self.length)]\n",
    "        random.shuffle(assignments)\n",
    "        fold_size = self.length//k #last fold will have extra items from excluded by rounding\n",
    "        self.folds = {}\n",
    "        for foldnum in range(k-2):\n",
    "            self.folds[foldnum] = assignments[fold_size*foldnum:fold_size*(foldnum+1)]\n",
    "        self.folds[k-1] = assignments[fold_size*(foldnum+1):]\n",
    "\n",
    "    def get_fold(self,fold_num):\n",
    "        assert len(self.folds.keys()) > 0\n",
    "        return self.__getitem__(self.folds[fold_num])#what if this ALSO returned a generator??\n",
    "# dataset = patient_dataset('ds007045')\n",
    "# dataset = patient_dataset('ds007156')\n",
    "dataset = patient_dataset('ds002424')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "203380e6-f672-4620-a4e9-bdef0e7c1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "dataset.generate_folds(10)\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba9ee520-3735-4a35-9d40-28c183e30110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.2917150139808653, 'Minutes for ', 7, ' Patients')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time.time()\n",
    "fold = dataset.get_fold(2)\n",
    "end = time.time()\n",
    "(end-start)/60, \"Minutes for \",len(fold),\" Patients\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f3ff9-a5fd-4827-a249-a2d40307d4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3dc294-1dbf-4f47-ba52-f0be5d2c3095",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "print(len([d['data'] for d in dataset[0:100]]))\n",
    "end = time.time()\n",
    "(end-start)/60,'Minutes for ',dataset.length,' images' #2.7min for scans and metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e51a8cd-363e-4adb-8ca7-bfbf3b18862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wilo 2/9: I have to rethink how im keeping/dropping items for RAM/disk\n",
    "#if the dataset is too large for the disk, its certainly too large for RAM. \n",
    "#so i definitely have to manage both.\n",
    "#i should NOT rely on fold size for memory management. That is bad\n",
    "#i should not wait until model-run time to dl get\n",
    "\n",
    "#what if i worked on a batching system that was unrelated to the folds\n",
    "#it would be assumed that you have 2x disc space than your batch size and x RAM to your batch size\n",
    "#and while one batch was being used, another is getting downloaded with datalad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd09b94-750c-42de-99e5-162ec14f517e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8c0782-ccf3-42d8-a29f-97bcb0f11fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The problem statement is: Make a dataloader for OpenNeuro where the dataset is larger than available disk (and RAM) space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2630dde9-b0bc-4913-a1ee-296e85302d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716c031-2d11-475d-b3c4-d57b8788dc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7c70ff-be98-4b5d-83f9-b54e7a5a03b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9808f310-2dca-48fc-9d1b-dc49f29644bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d96f0-5276-46d4-8889-a02d1bc0fff4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913ea0e9-da67-4b02-8796-b4b2a27617a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf754b0-81bb-48ef-ab9a-7943e64bb007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591524a0-4b18-41ff-8a26-1b08dc20cb7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
